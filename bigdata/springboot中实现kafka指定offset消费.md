[è¿”å›ä¸Šå±‚](index)

# springbootä¸­å®ç°kafkaæŒ‡å®šoffsetæ¶ˆè´¹

kafkaæ¶ˆè´¹è¿‡ç¨‹éš¾å…ä¼šé‡åˆ°éœ€è¦é‡æ–°æ¶ˆè´¹çš„åœºæ™¯ï¼Œä¾‹å¦‚æˆ‘ä»¬æ¶ˆè´¹åˆ°kafkaæ•°æ®ä¹‹åéœ€è¦è¿›è¡Œå­˜åº“æ“ä½œï¼Œè‹¥æŸä¸€æ—¶åˆ»æ•°æ®åº“downäº†ï¼Œå¯¼è‡´kafkaæ¶ˆè´¹çš„æ•°æ®æ— æ³•å…¥åº“ï¼Œä¸ºäº†å¼¥è¡¥æ•°æ®åº“downæœŸé—´çš„æ•°æ®æŸå¤±ï¼Œæœ‰ä¸€ç§åšæ³•æˆ‘ä»¬å¯ä»¥æŒ‡å®škafkaæ¶ˆè´¹è€…çš„offsetåˆ°ä¹‹å‰æŸä¸€æ—¶é—´çš„æ•°å€¼ï¼Œç„¶åé‡æ–°è¿›è¡Œæ¶ˆè´¹ã€‚

##é¦–å…ˆåˆ›å»ºkafkaæ¶ˆè´¹æœåŠ¡
```java
@Service
@Slf4j
//å®ç°CommandLineRunneræ¥å£ï¼Œåœ¨springbootå¯åŠ¨æ—¶è‡ªåŠ¨è¿è¡Œå…¶runæ–¹æ³•ã€‚
public class TspLogbookAnalysisService implements CommandLineRunner {
    @Override
    public void run(String... args) {
        //do something
    }
}
```

##kafkaæ¶ˆè´¹æ¨¡å‹å»ºç«‹

kafka serverä¸­æ¯ä¸ªä¸»é¢˜å­˜åœ¨å¤šä¸ªåˆ†åŒºï¼ˆpartitionï¼‰ï¼Œæ¯ä¸ªåˆ†åŒºè‡ªå·±ç»´æŠ¤ä¸€ä¸ªåç§»é‡ï¼ˆoffsetï¼‰ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å®ç°kafka consumeræŒ‡å®šoffsetæ¶ˆè´¹ã€‚

åœ¨è¿™é‡Œä½¿ç”¨consumer-->partitionä¸€å¯¹ä¸€çš„æ¶ˆè´¹æ¨¡å‹ï¼Œæ¯ä¸ªconsumerå„è‡ªç®¡ç†è‡ªå·±çš„partitionã€‚


![kafka consumer partition](http://dxsn-1300740068.cos.ap-nanjing.myqcloud.com/2021-12-12-042032.png)



```java
@Service
@Slf4j
public class TspLogbookAnalysisService implements CommandLineRunner {
    //å£°æ˜kafkaåˆ†åŒºæ•°ç›¸ç­‰çš„æ¶ˆè´¹çº¿ç¨‹æ•°ï¼Œä¸€ä¸ªåˆ†åŒºå¯¹åº”ä¸€ä¸ªæ¶ˆè´¹çº¿ç¨‹
    private  static final int consumeThreadNum = 9;
    //ç‰¹æ®ŠæŒ‡å®šæ¯ä¸ªåˆ†åŒºå¼€å§‹æ¶ˆè´¹çš„offset
    private List<Long> partitionOffsets = Lists.newArrayList(1111,1112,1113,1114,1115,1116,1117,1118,1119);
   
    private ExecutorService executorService = Executors.newFixedThreadPool(consumeThreadNum);

    @Override
    public void run(String... args) {
        //å¾ªç¯éå†åˆ›å»ºæ¶ˆè´¹çº¿ç¨‹
        IntStream.range(0, consumeThreadNum)
                .forEach(partitionIndex -> executorService.submit(() -> startConsume(partitionIndex)));
    }
}
```

##kafka consumerå¯¹offsetçš„å¤„ç†

###å£°æ˜kafka consumerçš„é…ç½®ç±»
```java
private Properties buildKafkaConfig() {
    Properties kafkaConfiguration = new Properties();
    kafkaConfiguration.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "");
    kafkaConfiguration.put(ConsumerConfig.GROUP_ID_CONFIG, "");
    kafkaConfiguration.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, "");
    kafkaConfiguration.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, "");
    kafkaConfiguration.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "");
    kafkaConfiguration.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "");
    kafkaConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"");
    kafkaConfiguration.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "");
    ...æ›´å¤šé…ç½®é¡¹

    return kafkaConfiguration;
}
```

###åˆ›å»ºkafka consumerï¼Œå¤„ç†offsetï¼Œå¼€å§‹æ¶ˆè´¹æ•°æ®ä»»åŠ¡

```java
private void startConsume(int partitionIndex) {
    //åˆ›å»ºkafka consumer
    KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<>(buildKafkaConfig());

    try {
        //æŒ‡å®šè¯¥consumerå¯¹åº”çš„æ¶ˆè´¹åˆ†åŒº
        TopicPartition partition = new TopicPartition(kafkaProperties.getKafkaTopic(), partitionIndex);
        consumer.assign(Lists.newArrayList(partition));

        //consumerçš„offsetå¤„ç†
        if (collectionUtils.isNotEmpty(partitionOffsets)  &&  partitionOffsets.size() == consumeThreadNum) {
            Long seekOffset = partitionOffsets.get(partitionIndex);
            log.info("partition:{} , offset seek from {}", partition, seekOffset);
            consumer.seek(partition, seekOffset);
        }
        
        //å¼€å§‹æ¶ˆè´¹æ•°æ®ä»»åŠ¡
        kafkaRecordConsume(consumer, partition);
    } catch (Exception e) {
        log.error("kafka consume error:{}", ExceptionUtils.getFullStackTrace(e));
    } finally {
        try {
            consumer.commitSync();
        } finally {
            consumer.close();
        }
    }
}
```

###æ¶ˆè´¹æ•°æ®é€»è¾‘ï¼Œoffsetæ“ä½œ

```java
private void kafkaRecordConsume(KafkaConsumer<String, byte[]> consumer, TopicPartition partition) {
    while (true) {
        try {
            ConsumerRecords<String, byte[]> records = consumer.poll(TspLogbookConstants.POLL_TIMEOUT);
            //å…·ä½“çš„å¤„ç†æµç¨‹
            records.forEach((k) -> handleKafkaInput(k.key(), k.value()));

            //ğŸŒ¿å¾ˆé‡è¦ï¼šæ—¥å¿—è®°å½•å½“å‰consumerçš„offsetï¼Œpartitionç›¸å…³ä¿¡æ¯(ä¹‹åå¦‚éœ€é‡æ–°æŒ‡å®šoffsetæ¶ˆè´¹å°±ä»è¿™é‡Œçš„æ—¥å¿—ä¸­è·å–offsetï¼Œpartitionä¿¡æ¯)
            if (records.count() > 0) {
                String currentOffset = String.valueOf(consumer.position(partition));
                log.info("current records size isï¼š{}, partition is: {}, offset is:{}", records.count(), consumer.assignment(), currentOffset);
            }
    
            //offsetæäº¤        
            consumer.commitAsync();
        } catch (Exception e) {
            log.error("handlerKafkaInput error{}", ExceptionUtils.getFullStackTrace(e));
        }
    }
}
```

![](http://dxsn-1300740068.cos.ap-nanjing.myqcloud.com/2021-12-12-042038.jpg)


---
---
---


## ğŸ¤”  ğŸ’­ ğŸ‘‡ğŸ‘‡ğŸ‘‡

<script src="https://utteranc.es/client.js"
        repo="dongxishaonian/issue-posted"
        issue-term="pathname"
        label="ğŸ™‚ğŸ™ƒğŸ˜¡ğŸ¥¶ğŸ˜¬ğŸ¤£ğŸ˜„"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

